<!DOCTYPE html><html lang="zh-tw" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Attention is all you need 筆記 | SineLin's Blog</title><meta name="description" content="Attention is all you need 筆記"><meta name="keywords" content="Attention is all you need 筆記, Attention is all you need, transformer"><meta name="author" content="SineLin"><meta name="copyright" content="SineLin"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="Attention is all you need 筆記"><meta name="twitter:description" content="Attention is all you need 筆記"><meta name="twitter:image" content="https://i.imgur.com/aALmU0P.png"><meta property="og:type" content="article"><meta property="og:title" content="Attention is all you need 筆記"><meta property="og:url" content="https://sinlin0908.github.io/2020/01/16/nlp_paper/transformer/"><meta property="og:site_name" content="SineLin's Blog"><meta property="og:description" content="Attention is all you need 筆記"><meta property="og:image" content="https://i.imgur.com/aALmU0P.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="canonical" href="https://sinlin0908.github.io/2020/01/16/nlp_paper/transformer/"><link rel="next" title="Effective Approaches to Attention-based Neural Machine Translation 筆記" href="/https:/sinlin0908.github.io/2019/12/19/nlp_paper/attention/"><link rel="dns-prefetch" href="https://www.google-analytics.com"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154354898-1', 'auto');
ga('send', 'pageview');
</script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'false',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'true',
  Snackbar: undefined
  
}</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-154354898-1"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-154354898-1');</script><meta name="generator" content="Hexo 4.1.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">SineLin's Blog</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">17</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">5</div></a></div></div><div class="mobile_data_item is_center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#前言"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">前言</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Why-we-need-self-attention"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">Why we need self-attention</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#RNN"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">RNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#CNN"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">CNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Self-attention"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">Self-attention</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Model"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Model</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Definition-of-Attention"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">Definition of Attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Scale-Dot-product-Attention"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">Scale Dot-product Attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Parallel-computing"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">Parallel computing</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Self-attention-1"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">Self-attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Multi-head-attention"><span class="toc_mobile_items-number">3.5.</span> <span class="toc_mobile_items-text">Multi-head attention</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Position-Embedding"><span class="toc_mobile_items-number">3.6.</span> <span class="toc_mobile_items-text">Position Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Encoder"><span class="toc_mobile_items-number">3.7.</span> <span class="toc_mobile_items-text">Encoder</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Decoder"><span class="toc_mobile_items-number">3.8.</span> <span class="toc_mobile_items-text">Decoder</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Result"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">Result</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Reference"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">Reference</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Paper"><span class="toc_mobile_items-number">5.1.</span> <span class="toc_mobile_items-text">Paper</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Website"><span class="toc_mobile_items-number">5.2.</span> <span class="toc_mobile_items-text">Website</span></a></li></ol></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-we-need-self-attention"><span class="toc-number">2.</span> <span class="toc-text">Why we need self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN"><span class="toc-number">2.1.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN"><span class="toc-number">2.2.</span> <span class="toc-text">CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-attention"><span class="toc-number">2.3.</span> <span class="toc-text">Self-attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model"><span class="toc-number">3.</span> <span class="toc-text">Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-of-Attention"><span class="toc-number">3.1.</span> <span class="toc-text">Definition of Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scale-Dot-product-Attention"><span class="toc-number">3.2.</span> <span class="toc-text">Scale Dot-product Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parallel-computing"><span class="toc-number">3.3.</span> <span class="toc-text">Parallel computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Self-attention-1"><span class="toc-number">3.4.</span> <span class="toc-text">Self-attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-head-attention"><span class="toc-number">3.5.</span> <span class="toc-text">Multi-head attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Position-Embedding"><span class="toc-number">3.6.</span> <span class="toc-text">Position Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-number">3.7.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-number">3.8.</span> <span class="toc-text">Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Result"><span class="toc-number">4.</span> <span class="toc-text">Result</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reference"><span class="toc-number">5.</span> <span class="toc-text">Reference</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Paper"><span class="toc-number">5.1.</span> <span class="toc-text">Paper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Website"><span class="toc-number">5.2.</span> <span class="toc-text">Website</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.imgur.com/aALmU0P.png)"><div id="post-info"><div id="post-title"><div class="posttitle">Attention is all you need 筆記</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2020-01-16<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2020-01-17</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Paper-Note/">Paper Note</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Paper-Note/NLP/">NLP</a></span><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>!!貼心提醒: 可以由右下角齒輪設定將文章轉成簡體字</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>如有錯誤，請留言提醒，謝謝 ^^</p>
<p><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" target="_blank" rel="noopener">Attention Is All You Need paper link</a></p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Facebook 推出 Convolutional Sequence to Sequence [1] 架構使用 CNN 平行運算的效果加速運算後，Google 緊接著推出 self-attention 的機制，只需要 attention 即可。</p>
<h2 id="Why-we-need-self-attention"><a href="#Why-we-need-self-attention" class="headerlink" title="Why we need self-attention"></a>Why we need self-attention</h2><p>以往大部分 machine translation task 是採用 RNN 的架構，像是 sequence to sequence [2,3]，之後 Facebook 提出 Convolutional Sequence to Sequence (ConvS2S)。以下會探討優缺點。</p>
<h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>RNN 雖然在生成生成不固定長度的 task 非常 powerful，像是語音、語句生成等等，但是他有一個問題是<code>不能平行運算</code>。</p>
<p><code>不能平行運算</code> 是怎麼回事?</p>
<p>在 RNN 架構中，當產生當下的 hidden state $h_{t}$ 時，必須等待 $h_{t-1}$ 計算完畢，因為 $h_{t}$ 的計算是有參考 $h_{t-1}$ (如公式)，因此才無法平行運算。</p>
<script type="math/tex; mode=display">
\boldsymbol{h}_{j}=f\left(\boldsymbol{h}_{j-1}, \boldsymbol{s}\right)</script><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><p>為了解決<code>平行運算</code>的問題，Facebook 提出 ConvS2S。 運用 CNN 可平行運算的特性 (如圖 1)。</p>
<p><img alt="ConvS2S" data-src="https://i.imgur.com/IazqtK2.png" class="lazyload"></p>
<center>圖 1、 ConvS2S</center>

<p>圖 1 中， ConvS2S 運用 CNN window 概念建立 word 和 word 之間的關係，並且學習到之間的 dependency。</p>
<p>CNN 沒辦法包括較遠的 word 資訊，因此使用 multi-layer 機制，疊多層 CNN 來解決這個問題。第二層計算第一層計算完的資訊……以此類推 (如圖 2)，不過缺點是記憶體使用量太多。</p>
<p><img alt="Multi-layer ConvS2S" data-src="https://i.imgur.com/GzsKVhW.png" class="lazyload"></p>
<center>圖 2、Multi-layer ConvS2S</center>

<h3 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>本篇論文只使用 attention 機制和 fully connected layer 來計算 sequence 的 representation。</p>
<p>attention 機制是一堆矩陣的運算，不僅可以平行運算，記憶體使用量也很少。</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>圖 3 是本篇論文所提出的 transformer 架構圖，紅色框框為 encoder，綠色框框為 decoder。</p>
<p><img alt="transformer" data-src="https://i.imgur.com/IXJCH9m.png" class="lazyload"></p>
<center>圖 3、transformer 架構圖</center>

<h3 id="Definition-of-Attention"><a href="#Definition-of-Attention" class="headerlink" title="Definition of Attention"></a>Definition of Attention</h3><p>本篇論文將 attention 分成 Q、K、V 三個要素</p>
<ul>
<li>Q : 去 match 別人</li>
<li>K : 被 match</li>
<li>V : 要提取的資訊</li>
</ul>
<p>而計算公式如下:</p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>與 ConvS2S 的 attention 做比較</p>
<p><img alt="ConvS2S attention 公式" data-src="https://i.imgur.com/eQaO1u3.png" class="lazyload"></p>
<center>圖 4、ConvS2S attention 公式</center>

<p>圖五是 attention 結構圖，對照公式可以更清楚的理解<br>其中 mask 是 decoder 才需要因為 decoder 並不需要知道未來的訊息</p>
<p><img alt="Scale Dot-product Attention" data-src="https://i.imgur.com/e6d6Qu7.png" class="lazyload"></p>
<center>圖 5、attention 結構</center>

<p>以台大李宏毅老師的影片圖解 transformer attention 的公式，attention 的概念 都跟 attention model[3] 差不多</p>
<p>如圖 6</p>
<ol>
<li>每個 x 都各自分成 Q、K、V 三個要素</li>
<li>Q 與每個 K (包含自己的)相乘得到 $\alpha$</li>
</ol>
<p><img alt="attention 公式圖解 1" data-src="https://i.imgur.com/kML3Op0.png" class="lazyload"></p>
<center>圖 6、Q 與 K match</center>

<p>圖 7<br>將 $\alpha$ 做 softmax 得 attention score，代表 X 在 target word 占多少比例</p>
<p><img alt="attention 公式圖解 2" data-src="https://i.imgur.com/VD5TA9u.png" class="lazyload"></p>
<center>圖 7、attention score</center>

<p>圖 8</p>
<p>將 attention score 與 V 做 weighted sum 得 context vector</p>
<p><img alt="attention 公式圖解 3" data-src="https://i.imgur.com/smhkxYl.png" class="lazyload"></p>
<center>圖 8、context vector</center>

<h3 id="Scale-Dot-product-Attention"><a href="#Scale-Dot-product-Attention" class="headerlink" title="Scale Dot-product Attention"></a>Scale Dot-product Attention</h3><p>在 self-attention 的公式中，論文將 ${Q K^{T}}$ 除以 $\sqrt{d_{k}}$ 來做 scale</p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>原因是當 $d_{k}$ 變大時，也就是說 ${Q K^{T}}$ 計算的 element 變多，${Q K^{T}}$ 的 variance 會上升。<br>導致:</p>
<ul>
<li>distribution 變 peak</li>
<li>層數越深的，performance 越差</li>
</ul>
<p>為什麼是 $\sqrt{d_{k}}$ 而不是 ${d_{k}}$? 論文沒有明確的說法</p>
<h3 id="Parallel-computing"><a href="#Parallel-computing" class="headerlink" title="Parallel computing"></a>Parallel computing</h3><p>利用矩陣運算來達到平行運算的效果<br>這邊也使用台大李宏毅老師的影片圖解平行運算(忽略 $\sqrt{d_{k}}$)</p>
<p>公式再寫一次</p>
<script type="math/tex; mode=display">
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>首先 ${Q K^{T}}$ 的部分</p>
<ul>
<li>將每個 $k$ 合併在一起形成 $K$</li>
<li>再合併 $q$ 形成 $Q$，與 $K$ 做內積</li>
</ul>
<p><img alt="平行計算-QK" data-src="https://i.imgur.com/bzB2vi7.png" class="lazyload"><br><img alt="平行計算-QK" data-src="https://i.imgur.com/BS9DgUC.png" class="lazyload"></p>
<center>圖 9、平行計算-QK</center>

<p>再來 ${Q K^{T}}$ 的結果與 $V$ 做矩陣相乘(等同於 weighted sum)得到 context vector</p>
<p><img alt="平行計算 context vector" data-src="https://i.imgur.com/kjLttFd.png" class="lazyload"></p>
<center>圖 10、平行計算-context vector</center>

<h3 id="Self-attention-1"><a href="#Self-attention-1" class="headerlink" title="Self-attention"></a>Self-attention</h3><p>運用 attention，Q、K、V 都是自己，也就是 $Attention(X,X,X)$， 來計算 sequence 中彼此之間的關係。(如圖 11)</p>
<p><img alt="self-attention" data-src="https://i.imgur.com/S1WRKOo.png" class="lazyload"></p>
<center>圖 11、self-attention</center>

<p>這個架構有點像 ConvS2S (如圖 12)</p>
<p><img alt="ConvS2S" data-src="https://i.imgur.com/IazqtK2.png" class="lazyload"></p>
<center>圖 12、 ConvS2S</center>

<h3 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h3><p>如之前講的 self-attention 類似 ConvS2S 的架構，但是 CNN 能學習到 word 和 word 之間的關係，像是哪個字代表 who 等等 (如圖 13)</p>
<p><img alt="CNN 能學習到 word 和 word 之間的關係" data-src="https://i.imgur.com/RtrX5Hc.png" class="lazyload"></p>
<center>圖 13、 CNN 能學習到 word 和 word 之間的關係</center>

<p>但是 self-attenion 由於是學習 attend 到哪一個 word，也就是說，self-attention 只能學習到單項程度上的差異而已，像是哪個 word 對於 who 的程度比較多 (如圖 14)，並不能學到多項關係。</p>
<p><img alt="self-attention word 和 word 之間的程度關係" data-src="https://i.imgur.com/LZTO7up.png" class="lazyload"><br><img alt="self-attention word 和 word 之間的程度關係" data-src="https://i.imgur.com/NG7c1tF.png" class="lazyload"></p>
<center>圖 14、 self-attention word 和 word 之間的程度關係，顏色最灰的代表 who</center>

<p>因此我們需要 multi-head self-attention。每一個 head 都代表著某一項關係，像是 who、where，再以 self-attention 選擇最好的那一個 (圖 15)</p>
<p><img alt="multi-head self-attention" data-src="https://i.imgur.com/1RIdyK0.png" class="lazyload"></p>
<center>圖 15、multi-head self-attention</center>

<p>(如圖 16) Multi-head 將 K、Q、V 經過 fully connected layer 分成多個 head input ，之後將 multi-head 的結果 concate 起來再丟入 fully connected layer， 這裡的 fully connected layer 我當作是一個 size 的轉換器。</p>
<p><img alt="multi-head 架構圖" data-src="https://i.imgur.com/m34SGBF.png" class="lazyload"></p>
<center>圖 16、multi-head 架構圖</center>

<p>公式如下<br><img alt="multi-head 公式" data-src="https://i.imgur.com/Ykl9EUr.png" class="lazyload"></p>
<h3 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h3><p>Position Embedding 是用來增加位置訊息，因為 self-attention 並不能項 RNN 一樣有時間序列的概念</p>
<p>在 ConvS2S 裡， position embedding 是用 neural network 訓練出來的，而這篇論文是以三角函式來計算，如下:</p>
<script type="math/tex; mode=display">
\begin{aligned}
P E_{(\text {pos}, 2 i)} &=\sin \left(\text {pos} / 10000^{2 i / d_{\text {madel }}}\right) \\
P E_{(\text {pos}, 2 i+1)} &=\cos \left(\text {pos} / 10000^{2 i / d_{\text {madel }}}\right)
\end{aligned}</script><p>將 $id$ 為 $p$ 的位置映射到 $d_{model}$ 維度的位置向量，這個向量的第 $i$ 個位置就是 $PE_{(pos,i)}$，奇數用 cosine，偶數用 sine。</p>
<p>本篇論文與 neural network 訓練出來的比較過，效果大同小異，由於可以減少模型的複雜度，因此使用公式的方法。</p>
<p>Position Embedding 本身是一個絕對位置的信息，但在 sentence 中，相對位置也很重要。 我們可以利用三角函數來表達位置 $p+k$</p>
<script type="math/tex; mode=display">
sin(α+β)=sinαcosβ+cosαsinβ\\
cos(α+β)=cosαcosβ−sinαsinβ</script><p>最後 position embedding 與 word embedding 相加，直覺上這樣會導致訊息上有所損失，但台大李弘毅教授在課堂上說: “concate 與相加結果其實大同小異” (如圖 17)。</p>
<p><img alt="position embedding" data-src="https://i.imgur.com/EhtqAn2.png" class="lazyload"></p>
<center>圖 17、Position embedding</center>

<p>concate 在 input 到 fully connected layer ，可以拆成 $W^Ix^i+ W^pp^i$，結果與原先方法 $a^i+e^i$ 差不多。</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><img alt="Encoder" data-src="https://i.imgur.com/gh8f2Iq.png" class="lazyload"></p>
<center>圖 18、Encoder</center>

<p>結構上，一個 encoder layer 有一個 multi-layer 和一個 fully connected layer。encoder 總共有 6 層 encoder layer，目的是把 source sentence 的句意學出來。</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p><img alt="Decoder" data-src="https://i.imgur.com/d8Y8Chz.png" class="lazyload"></p>
<center>圖 19、Decoder</center>

<p>結構上，和 encoder 差不多，一個 Decoder layer 有一個 multi-head attention 和一個 fully connected layer，但多加了一個 multi-head attention 用來計算 encoder output 與 decoder 的關係。decoder 也總共有 6 層 decoder layer，</p>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p><img alt="transformer result" data-src="https://i.imgur.com/5BSbeAi.png" class="lazyload"></p>
<center>圖 20、transformer result</center>

<p><img alt="transformer 分析" data-src="https://i.imgur.com/nrnCVf9.png" class="lazyload"></p>
<center>圖 21、transformer 參數分析</center>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h3><p>[1] Gehring et al. : Convolutional sequence to sequence learning <a href="https://arxiv.org/pdf/1705.03122.pdf" target="_blank" rel="noopener">link</a><br>[2] Sutskever et al. : Sequence to Sequence Learning with Neural Networks <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener">link</a><br>[3] Luong et al. : Effective Approaches to Attention-based Neural Machine Translation, <a href="https://www.aclweb.org/anthology/D15-1166.pdf" target="_blank" rel="noopener">link</a></p>
<h3 id="Website"><a href="#Website" class="headerlink" title="Website"></a>Website</h3><ol>
<li><a href="https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ" target="_blank" rel="noopener">台大李弘毅老師</a></li>
<li><a href="https://www.youtube.com/channel/UCyB2RBqKbxDPGCs1PokeUiA" target="_blank" rel="noopener">台大陳蘊儂老師</a></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">SineLin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="/https:/sinlin0908.github.io/2020/01/16/nlp_paper/transformer/">https://sinlin0908.github.io/2020/01/16/nlp_paper/transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Paper/">Paper    </a><a class="post-meta__tags" href="/tags/NLP/">NLP    </a><a class="post-meta__tags" href="/tags/machine-translation/">machine translation    </a></div><div class="post_share"><div class="social-share" data-image="https://i.imgur.com/aALmU0P.png" data-sites="facebook,twitter"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/12/19/nlp_paper/attention/"><img class="next_cover lazyload" data-src="https://i.imgur.com/awmospk.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Effective Approaches to Attention-based Neural Machine Translation 筆記</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/12/19/nlp_paper/attention/" title="Effective Approaches to Attention-based Neural Machine Translation 筆記"><img class="relatedPosts_cover lazyload"data-src="https://i.imgur.com/awmospk.png"><div class="relatedPosts_title">Effective Approaches to Attention-based Neural Machine Translation 筆記</div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'https://sinlin0908.github.io/2020/01/16/nlp_paper/transformer/';
  this.page.identifier = '2020/01/16/nlp_paper/transformer/';
  this.page.title = 'Attention is all you need 筆記';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'SineLin-blog' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By SineLin</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">welcome to my blog</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="/js/third-party/canvas-nest.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>